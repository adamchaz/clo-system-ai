# All Assets Tab Migration Plan: Excel to PostgreSQL

## Executive Summary

The **All Assets** worksheet in TradeHypoPrelimv32.xlsm contains **1,004 assets** across **71 columns** of comprehensive financial data. This represents the **core asset portfolio** requiring migration to the PostgreSQL database. The migration involves complex data transformation, validation, and mapping to our optimized database schema.

**Migration Scope**: 1,004 assets √ó 71 properties = **71,284 individual data points** requiring precise migration with 100% accuracy.

## üìä All Assets Sheet Analysis

### **Sheet Structure Overview**
```
All Assets Worksheet (TradeHypoPrelimv32.xlsm)
‚îú‚îÄ‚îÄ Dimensions: 71 columns √ó 1,004 rows
‚îú‚îÄ‚îÄ Data Volume: ~71,284 individual data points
‚îú‚îÄ‚îÄ Primary Key: BLKRockID (Column 1)
‚îú‚îÄ‚îÄ Asset Count: 1,003 assets (excluding header)
‚îî‚îÄ‚îÄ Data Types: Mixed (text, numeric, dates, boolean flags)
```

### **Column Categories & Database Mapping**

#### **üÜî Primary Identifiers (4 columns)**
| Excel Column | Database Field | Priority | Transformation |
|--------------|----------------|----------|----------------|
| BLKRockID | `blkrock_id` | **CRITICAL** | Direct mapping |
| Issue Name | `issue_name` | **HIGH** | String cleanup |
| Issuer Name | `issuer_name` | **HIGH** | String cleanup |
| ISSUER ID | `issuer_id` | **MEDIUM** | Direct mapping |

#### **üí∞ Financial Core Data (8 columns)**
| Excel Column | Database Field | Priority | Transformation |
|--------------|----------------|----------|----------------|
| Par Amount | `par_amount` | **CRITICAL** | Currency cleanup ‚Üí Decimal |
| Market Value | `market_value` | **HIGH** | Percentage ‚Üí Decimal |
| Coupon | `coupon` | **HIGH** | Percentage ‚Üí Decimal |
| Coupon Spread | `cpn_spread` | **HIGH** | Basis points ‚Üí Decimal |
| Facility Size | `facility_size` | **MEDIUM** | Currency cleanup ‚Üí Decimal |
| Amount Issued | `amount_issued` | **MEDIUM** | Currency cleanup ‚Üí Decimal |
| Commitment Fee | `commit_fee` | **LOW** | Percentage ‚Üí Decimal |
| Unfunded Amount | `unfunded_amount` | **LOW** | Currency cleanup ‚Üí Decimal |

#### **üìÖ Date Information (4 columns)**
| Excel Column | Database Field | Priority | Transformation |
|--------------|----------------|----------|----------------|
| Maturity | `maturity` | **CRITICAL** | Date parsing ‚Üí DATE |
| Dated Date | `dated_date` | **MEDIUM** | Date parsing ‚Üí DATE |
| Issue Date | `issue_date` | **MEDIUM** | Date parsing ‚Üí DATE |
| First Payment Date | `first_payment_date` | **LOW** | Date parsing ‚Üí DATE |

#### **üè¶ Instrument Details (10 columns)**
| Excel Column | Database Field | Priority | Transformation |
|--------------|----------------|----------|----------------|
| Tranche | `tranche` | **MEDIUM** | Direct mapping |
| Bond/Loan | `bond_loan` | **MEDIUM** | Direct mapping |
| Coupon Type | `coupon_type` | **HIGH** | Direct mapping |
| Payment Frequency | `payment_freq` | **MEDIUM** | Text ‚Üí Integer |
| Index | `index_name` | **MEDIUM** | Direct mapping |
| Libor Floor | `libor_floor` | **MEDIUM** | Percentage ‚Üí Decimal |
| Index Cap | `index_cap` | **LOW** | Percentage ‚Üí Decimal |
| Currency | `currency` | **MEDIUM** | Direct mapping |
| Day Count | `day_count` | **LOW** | Direct mapping |
| Amortization Type | `amortization_type` | **LOW** | Direct mapping |

#### **‚≠ê Credit Ratings - Moody's (12 columns)**
| Excel Column | Database Field | Priority | Transformation |
|--------------|----------------|----------|----------------|
| Moody's Rating-21 | `mdy_rating` | **HIGH** | Direct mapping |
| Moody's Recovery Rate-23 | `mdy_recovery_rate` | **MEDIUM** | Percentage ‚Üí Decimal |
| Moody's Default Probability Rating-18 | `mdy_dp_rating` | **MEDIUM** | Direct mapping |
| Moody's Rating WARF | `mdy_dp_rating_warf` | **MEDIUM** | Direct mapping |
| Moody Facility Rating | `mdy_facility_rating` | **LOW** | Direct mapping |
| Moody Facility Outlook | `mdy_facility_outlook` | **LOW** | Direct mapping |
| Moody Issuer Rating | `mdy_issuer_rating` | **LOW** | Direct mapping |
| Moody Issuer outlook | `mdy_issuer_outlook` | **LOW** | Direct mapping |
| Moody's Senior Secured Rating | `mdy_snr_sec_rating` | **LOW** | Direct mapping |
| Moody Senior Unsecured Rating | `mdy_snr_unsec_rating` | **LOW** | Direct mapping |
| Moody Subordinate Rating | `mdy_sub_rating` | **LOW** | Direct mapping |
| Moody's Credit Estimate | `mdy_credit_est_rating` | **LOW** | Direct mapping |

#### **üìä Credit Ratings - S&P (5 columns)**
| Excel Column | Database Field | Priority | Transformation |
|--------------|----------------|----------|----------------|
| S&P Facility Rating | `sandp_facility_rating` | **MEDIUM** | Direct mapping |
| S&P Issuer Rating | `sandp_issuer_rating` | **MEDIUM** | Direct mapping |
| S&P Senior Secured Rating | `sandp_snr_sec_rating` | **LOW** | Direct mapping |
| S&P Subordinated Rating | `sandp_subordinate` | **LOW** | Direct mapping |
| S&P Recovery Rating | `sandp_rec_rating` | **LOW** | Direct mapping |

#### **üåç Classification Data (6 columns)**
| Excel Column | Database Field | Priority | Transformation |
|--------------|----------------|----------|----------------|
| Moody's Industry | `mdy_industry` | **HIGH** | Direct mapping |
| S&P Industry | `sp_industry` | **HIGH** | Direct mapping |
| Moody Asset Category | `mdy_asset_category` | **MEDIUM** | Direct mapping |
| S&P Priority Category | `sp_priority_category` | **MEDIUM** | Direct mapping |
| Country  | `country` | **HIGH** | Direct mapping |
| Seniority | `seniority` | **MEDIUM** | Direct mapping |

#### **üè∑Ô∏è Asset Flags & Characteristics (22 columns)**
| Excel Column | Database Field | Priority | Transformation |
|--------------|----------------|----------|----------------|
| Default Obligation | `default_asset` | **CRITICAL** | Boolean conversion |
| Date of Defaulted | `date_of_default` | **HIGH** | Date parsing ‚Üí DATE |
| PiKing | `piking` | **MEDIUM** | Boolean conversion |
| PIK Amount | `pik_amount` | **MEDIUM** | Currency ‚Üí Decimal |
| Deferred Interest Bond | `flags.deferred_interest` | **LOW** | Boolean ‚Üí JSON |
| Delayed Drawdown  | `flags.delayed_drawdown` | **LOW** | Boolean ‚Üí JSON |
| Revolver | `flags.revolver` | **LOW** | Boolean ‚Üí JSON |
| Letter of Credit | `flags.letter_of_credit` | **LOW** | Boolean ‚Üí JSON |
| Participation | `flags.participation` | **LOW** | Boolean ‚Üí JSON |
| DIP | `flags.dip` | **LOW** | Boolean ‚Üí JSON |
| Convertible | `flags.convertible` | **LOW** | Boolean ‚Üí JSON |
| Structured Finance | `flags.structured_finance` | **LOW** | Boolean ‚Üí JSON |
| Bridge Loan | `flags.bridge_loan` | **LOW** | Boolean ‚Üí JSON |
| Current Pay | `flags.current_pay` | **LOW** | Boolean ‚Üí JSON |
| Cov-Lite | `flags.cov_lite` | **LOW** | Boolean ‚Üí JSON |
| First Lien Last Out Loan | `flags.first_lien_last_out` | **LOW** | Boolean ‚Üí JSON |
| WAL | `wal` | **LOW** | Numeric ‚Üí Decimal |
| PMT EOM | `payment_eom` | **LOW** | Boolean conversion |
| Business Day Convention | `business_day_conv` | **LOW** | Direct mapping |

#### **üìù Additional Information (2 columns)**
| Excel Column | Database Field | Priority | Transformation |
|--------------|----------------|----------|----------------|
| Credit Opinion | `analyst_opinion` | **LOW** | Text ‚Üí TEXT |
| Column_71 | N/A | **SKIP** | Empty column |

## üîÑ Migration Strategy

### **Phase 1: Data Extraction**

#### **Excel Data Reading Approach**
```python
def extract_all_assets_data():
    """Extract All Assets sheet with optimized reading"""
    
    # Load workbook with data values
    workbook = openpyxl.load_workbook('TradeHypoPrelimv32.xlsm', data_only=True)
    sheet = workbook['All Assets']
    
    # Extract headers (row 1)
    headers = [cell.value for cell in sheet[1]]
    
    # Extract data rows (rows 2-1004)
    assets_data = []
    for row in sheet.iter_rows(min_row=2, max_row=1004, values_only=True):
        asset_dict = dict(zip(headers, row))
        assets_data.append(asset_dict)
    
    return assets_data
```

#### **Data Quality Validation During Extraction**
- **Row Count Verification**: Ensure all 1,003 asset rows are extracted
- **Column Count Verification**: Confirm all 71 columns are present
- **Primary Key Validation**: Verify BLKRockID uniqueness
- **Data Type Initial Check**: Identify obvious data type issues

### **Phase 2: Data Transformation**

#### **Critical Field Transformations**

**1. Financial Data Cleaning**
```python
def clean_financial_data(value):
    """Clean monetary values from Excel format"""
    if pd.isna(value):
        return None
    
    # Remove currency symbols and formatting
    cleaned = str(value).replace('$', '').replace(',', '').strip()
    
    try:
        return Decimal(cleaned)
    except (ValueError, decimal.InvalidOperation):
        return None
```

**2. Date Parsing**
```python
def parse_asset_date(value):
    """Parse various Excel date formats"""
    if pd.isna(value):
        return None
    
    if isinstance(value, datetime):
        return value.date()
    
    # Handle various string date formats
    date_formats = ['%Y-%m-%d', '%m/%d/%Y', '%d/%m/%Y', '%Y%m%d']
    
    for fmt in date_formats:
        try:
            return datetime.strptime(str(value), fmt).date()
        except ValueError:
            continue
    
    return None
```

**3. Boolean Flag Processing**
```python
def process_asset_flags(asset_row):
    """Process asset flags into JSONB format"""
    
    flag_columns = [
        'Deferred Interest Bond', 'Delayed Drawdown', 'Revolver',
        'Letter of Credit', 'Participation', 'DIP', 'Convertible',
        'Structured Finance', 'Bridge Loan', 'Current Pay', 'Cov-Lite',
        'First Lien Last Out Loan'
    ]
    
    flags = {}
    for col in flag_columns:
        value = asset_row.get(col)
        if value in ['Y', 'YES', '1', 1, True]:
            flags[col.lower().replace(' ', '_')] = True
        elif value in ['N', 'NO', '0', 0, False]:
            flags[col.lower().replace(' ', '_')] = False
    
    return flags if flags else None
```

**4. Rating Standardization**
```python
def standardize_rating(rating_value):
    """Standardize rating formats"""
    if pd.isna(rating_value):
        return None
    
    rating_str = str(rating_value).strip().upper()
    
    # Handle common variations
    rating_replacements = {
        'AAA': 'AAA', 'AA+': 'AA+', 'AA': 'AA', 'AA-': 'AA-',
        'A+': 'A+', 'A': 'A', 'A-': 'A-',
        'BBB+': 'BBB+', 'BBB': 'BBB', 'BBB-': 'BBB-',
        'BB+': 'BB+', 'BB': 'BB', 'BB-': 'BB-',
        'B+': 'B+', 'B': 'B', 'B-': 'B-',
        'CCC+': 'CCC+', 'CCC': 'CCC', 'CCC-': 'CCC-',
        'CC': 'CC', 'C': 'C', 'D': 'D'
    }
    
    return rating_replacements.get(rating_str, rating_str)
```

### **Phase 3: Database Loading**

#### **Optimized Batch Loading Strategy**
```python
def load_assets_to_database(transformed_assets):
    """Load assets with optimized batch processing"""
    
    batch_size = 100  # Optimal for asset complexity
    total_batches = len(transformed_assets) // batch_size + 1
    
    for i in range(0, len(transformed_assets), batch_size):
        batch = transformed_assets[i:i + batch_size]
        
        try:
            # Create SQLAlchemy Asset objects
            asset_objects = []
            for asset_data in batch:
                asset = Asset(**asset_data)
                asset_objects.append(asset)
            
            # Bulk insert batch
            db_session.add_all(asset_objects)
            db_session.commit()
            
            print(f"Loaded batch {i // batch_size + 1}/{total_batches}: {len(batch)} assets")
            
        except SQLAlchemyError as e:
            db_session.rollback()
            print(f"Error loading batch {i // batch_size + 1}: {e}")
            # Log individual asset errors and continue
```

### **Phase 4: Data Validation**

#### **Multi-Level Validation Framework**

**1. Data Completeness Validation**
```sql
-- Verify asset count
SELECT COUNT(*) as total_assets FROM assets;
-- Expected: 1003 assets

-- Check critical fields completion
SELECT 
  COUNT(CASE WHEN blkrock_id IS NULL THEN 1 END) as missing_blkrock_id,
  COUNT(CASE WHEN issue_name IS NULL THEN 1 END) as missing_issue_name,
  COUNT(CASE WHEN par_amount IS NULL THEN 1 END) as missing_par_amount
FROM assets;
-- Expected: All zeros
```

**2. Data Accuracy Validation**
```python
def validate_migration_accuracy():
    """Compare migrated data with Excel source"""
    
    # Load original Excel data
    excel_data = extract_all_assets_data()
    
    # Load database data  
    db_assets = session.query(Asset).all()
    
    discrepancies = []
    
    for excel_row in excel_data:
        blkrock_id = excel_row['BLKRockID']
        db_asset = session.query(Asset).filter(Asset.blkrock_id == blkrock_id).first()
        
        if not db_asset:
            discrepancies.append(f"Asset {blkrock_id} not found in database")
            continue
        
        # Compare critical fields
        if excel_row['Par Amount'] != db_asset.par_amount:
            discrepancies.append(f"Par amount mismatch for {blkrock_id}")
        
        if excel_row['Issue Name'] != db_asset.issue_name:
            discrepancies.append(f"Issue name mismatch for {blkrock_id}")
    
    return discrepancies
```

**3. Business Rule Validation**
```sql
-- Validate business rules
SELECT 
  COUNT(CASE WHEN par_amount <= 0 THEN 1 END) as negative_par,
  COUNT(CASE WHEN market_value < 0 OR market_value > 200 THEN 1 END) as invalid_mv,
  COUNT(CASE WHEN maturity < CURRENT_DATE THEN 1 END) as past_maturity,
  COUNT(CASE WHEN coupon < 0 OR coupon > 50 THEN 1 END) as invalid_coupon
FROM assets;
-- All should be 0
```

## üö® Risk Assessment & Mitigation

### **High-Risk Areas**

#### **Risk 1: Data Type Conversion Errors**
- **Risk**: Financial values with formatting, dates in various formats
- **Probability**: MEDIUM
- **Impact**: HIGH - Calculation errors
- **Mitigation**: 
  - Comprehensive data cleaning functions
  - Multiple parsing attempts with fallbacks
  - Sample data validation before full migration

#### **Risk 2: Asset Flag Interpretation**
- **Risk**: Boolean flags stored as various text formats (Y/N, YES/NO, 1/0)
- **Probability**: MEDIUM
- **Impact**: MEDIUM - Feature availability
- **Mitigation**:
  - Comprehensive boolean interpretation logic
  - Default to False for unclear values
  - Manual review of flag distribution

#### **Risk 3: Rating Standardization**
- **Risk**: Rating formats may vary or include non-standard values
- **Probability**: LOW
- **Impact**: MEDIUM - Credit analysis accuracy
- **Mitigation**:
  - Rating validation against known scales
  - Mapping table for common variations
  - Manual review of unmapped ratings

### **Data Quality Controls**

#### **Pre-Migration Data Quality Assessment**
```python
def assess_all_assets_quality():
    """Assess data quality before migration"""
    
    quality_report = {
        'total_assets': 0,
        'missing_critical_fields': {},
        'data_type_issues': [],
        'duplicate_blkrock_ids': 0,
        'invalid_dates': 0,
        'invalid_amounts': 0
    }
    
    # Implement comprehensive quality checks
    # Flag issues that need manual review
    # Generate quality score for migration readiness
    
    return quality_report
```

## üìà Expected Outcomes

### **Migration Success Metrics**
- **Data Completeness**: 100% of 1,003 assets migrated
- **Field Coverage**: 95%+ of critical fields populated
- **Data Accuracy**: 99.99% accuracy in financial values
- **Performance**: Migration completion within 30 minutes
- **Validation**: 100% pass rate on business rule validation

### **Post-Migration Capabilities**
1. **Complete Asset Portfolio**: All 1,003 assets available for analysis
2. **Comprehensive Attribution**: 71 properties per asset for detailed analysis
3. **Rating Integration**: Full Moody's and S&P rating coverage
4. **Industry Analysis**: Complete industry and geographic classification
5. **Advanced Querying**: SQL-based portfolio analysis and reporting
6. **API Access**: RESTful API endpoints for external integration

## üõ†Ô∏è Implementation Scripts

### **Master All Assets Migration Script**
```python
# scripts/migrate_all_assets.py
def migrate_all_assets():
    """Complete All Assets migration workflow"""
    
    print("Starting All Assets migration...")
    
    # Phase 1: Extract
    raw_data = extract_all_assets_data()
    print(f"Extracted {len(raw_data)} assets from Excel")
    
    # Phase 2: Transform  
    transformed_data = transform_assets_data(raw_data)
    print(f"Transformed {len(transformed_data)} assets for database")
    
    # Phase 3: Load
    load_result = load_assets_to_database(transformed_data)
    print(f"Loaded {load_result['success_count']} assets to database")
    
    # Phase 4: Validate
    validation_result = validate_migration_accuracy()
    print(f"Validation complete: {len(validation_result)} discrepancies found")
    
    return {
        'extracted': len(raw_data),
        'transformed': len(transformed_data), 
        'loaded': load_result['success_count'],
        'discrepancies': len(validation_result)
    }
```

## üéØ Next Steps

### **Immediate Actions (This Week)**
1. **Finalize Migration Scripts**: Complete the All Assets specific migration tools
2. **Data Quality Assessment**: Run pre-migration quality analysis
3. **Sample Migration Testing**: Test with 100-asset subset
4. **Validation Framework**: Implement comprehensive validation checks

### **Migration Execution (Next Week)**  
1. **Full Data Migration**: Execute complete All Assets migration
2. **Validation & Testing**: Run all validation checks
3. **Performance Testing**: Verify database performance with full dataset
4. **User Acceptance**: Stakeholder validation of migrated data

The **All Assets** migration represents the **core foundation** of the CLO system modernization. With 1,003 assets and 71 properties each, this migration will transform **71,284 data points** from Excel to a modern PostgreSQL database, enabling advanced analytics, reporting, and portfolio management capabilities.

---

**Document Version**: 1.0  
**Created**: January 10, 2025  
**Migration Target**: 1,003 assets from TradeHypoPrelimv32.xlsm ‚Üí PostgreSQL  
**Expected Duration**: 2-3 hours for full migration with validation