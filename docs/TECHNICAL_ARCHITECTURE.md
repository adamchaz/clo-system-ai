# CLO Management System - Technical Architecture

## Table of Contents

1. [Architecture Overview](#architecture-overview)
2. [System Components](#system-components)
3. [Data Architecture](#data-architecture)
4. [Application Architecture](#application-architecture)
5. [Integration Patterns](#integration-patterns)
6. [Security Architecture](#security-architecture)
7. [Deployment Architecture](#deployment-architecture)
8. [Performance Considerations](#performance-considerations)

---

## Architecture Overview

The CLO Management System follows a modern, microservices-oriented architecture with clear separation of concerns, enabling scalable portfolio management and risk analytics.

### Architectural Principles

- **Separation of Concerns**: Clear boundaries between data, business logic, and presentation layers
- **Scalability**: Horizontally scalable components with load balancing capabilities  
- **Reliability**: Fault-tolerant design with comprehensive error handling and recovery
- **Security**: Defense-in-depth security model with authentication, authorization, and audit trails
- **Maintainability**: Clean code architecture with comprehensive documentation and testing

### High-Level Architecture

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Frontend      │    │   API Gateway   │    │   Monitoring    │
│   (React SPA)   │◄──►│   (FastAPI)     │◄──►│   (Grafana)     │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                                │
                                ▼
                    ┌─────────────────────────┐
                    │   Business Logic Layer  │
                    │   - Authentication      │
                    │   - Portfolio Management│
                    │   - Risk Analytics      │
                    │   - Waterfall Calc      │
                    └─────────────────────────┘
                                │
                ┌───────────────┴───────────────┐
                ▼                               ▼
    ┌─────────────────┐              ┌─────────────────┐
    │ Operational DB  │              │  Migrated Data  │
    │ (PostgreSQL)    │              │  (SQLite DBs)   │
    │ - Portfolios    │              │  - Assets       │
    │ - Users         │              │  - Correlations │
    │ - Audit Logs    │              │  - Scenarios    │
    └─────────────────┘              └─────────────────┘
                │                               │
                └───────────────┬───────────────┘
                                ▼
                      ┌─────────────────┐
                      │  Cache Layer    │
                      │  (Redis)        │
                      └─────────────────┘
```

### Technology Stack

| Layer | Technologies |
|-------|-------------|
| Frontend | React 18, TypeScript, Material-UI, Recharts |
| API Gateway | FastAPI, Pydantic, SQLAlchemy |
| Business Logic | Python 3.9+, pandas, numpy, scipy |
| Databases | PostgreSQL 13+, SQLite 3, Redis 6+ |
| Infrastructure | Docker, Docker Compose, nginx |
| Monitoring | Prometheus, Grafana, ELK Stack |

---

## System Components

### 1. Frontend Layer (Planned - Phase 3)

#### React Single Page Application
- **Framework**: React 18 with functional components and hooks
- **State Management**: Redux Toolkit for complex state, Context API for authentication
- **UI Framework**: Material-UI for consistent design system
- **Routing**: React Router for client-side routing
- **Data Visualization**: Recharts for financial charts and analytics

#### Component Architecture
```
src/
├── components/           # Reusable UI components
│   ├── common/          # Shared components
│   ├── portfolio/       # Portfolio-specific components
│   ├── risk/           # Risk analytics components
│   └── charts/         # Data visualization components
├── pages/              # Route-level components
├── hooks/              # Custom React hooks
├── services/           # API integration services
├── store/              # Redux store configuration
└── utils/              # Utility functions
```

### 2. API Gateway Layer

#### FastAPI Application
```python
# Main application structure
backend/app/
├── main.py                 # Application entry point
├── core/                   # Core configuration
│   ├── config.py          # Settings management
│   ├── database_config.py # Database connections
│   └── security.py        # Security utilities
├── api/v1/                # API routes
│   ├── endpoints/         # Route handlers
│   └── dependencies.py    # Dependency injection
├── models/                # Database models
├── schemas/               # Pydantic models
├── services/              # Business logic services
└── utils/                 # Utility functions
```

#### API Design Patterns
- **RESTful Design**: Standard HTTP methods with resource-based URLs
- **Pydantic Validation**: Request/response validation with type safety
- **Dependency Injection**: Clean separation of concerns
- **Error Handling**: Consistent error responses with proper HTTP status codes

### 3. Business Logic Layer

#### Service Architecture
```python
# Service layer organization
services/
├── auth_service.py         # Authentication & authorization
├── portfolio_service.py    # Portfolio management
├── waterfall_service.py    # Waterfall calculations
├── risk_service.py         # Risk analytics
├── scenario_service.py     # Scenario analysis
├── data_integration.py     # Data integration layer
└── monitoring_service.py   # System monitoring
```

#### Key Services

**Authentication Service** (`auth_service.py`)
- JWT-based authentication
- Role-based access control (RBAC)
- Session management
- Password security with bcrypt

**Portfolio Service** (`portfolio_service.py`)  
- Portfolio CRUD operations
- Asset allocation management
- Performance calculations
- Compliance monitoring

**Waterfall Service** (`waterfall_service.py`)
- CLO payment waterfall calculations
- Multi-scenario stress testing
- Cash flow projections
- Payment priority management

**Risk Service** (`risk_service.py`)
- Value at Risk (VaR) calculations
- Monte Carlo simulations
- Correlation analysis
- Concentration risk metrics

### 4. Data Integration Layer

#### Purpose
Bridges migrated SQLite databases with operational PostgreSQL database, providing unified data access patterns.

#### Implementation
```python
# Data integration service
class DataIntegrationService:
    def __init__(self):
        self.sqlite_connections = self._init_sqlite_connections()
        self.postgresql_session = self._get_postgresql_session()
        
    def sync_assets_to_operational(self):
        """Synchronize asset data to operational database"""
        
    def get_asset_correlations(self, borrower1: str, borrower2: str):
        """Retrieve correlation data from SQLite"""
        
    def get_mag_scenario_parameters(self, scenario: str):
        """Access MAG scenario data"""
```

---

## Data Architecture

### Database Strategy

#### Multi-Database Architecture
The system employs a hybrid database strategy optimized for different data access patterns:

1. **SQLite Databases** (Migrated Data)
   - **Purpose**: Store migrated Excel data in read-optimized format
   - **Databases**: 5 specialized databases (assets, correlations, scenarios, config, reference)
   - **Access Pattern**: Read-heavy, batch processing
   - **Benefits**: Fast file-based access, no server overhead

2. **PostgreSQL Database** (Operational Data)
   - **Purpose**: Transactional operations, user data, audit logs
   - **Tables**: 47+ operational tables
   - **Access Pattern**: CRUD operations, real-time queries
   - **Benefits**: ACID compliance, concurrent access, advanced features

3. **Redis Cache**
   - **Purpose**: Performance optimization, session storage
   - **Data**: Frequently accessed calculations, user sessions
   - **Benefits**: Sub-millisecond latency, automatic expiration

### Data Model Design

#### Operational Database Schema (PostgreSQL)

```sql
-- Core business entities
CREATE TABLE portfolios (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(100) NOT NULL,
    deal_type VARCHAR(50),
    closing_date DATE,
    legal_maturity DATE,
    total_commitments DECIMAL(15,2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) UNIQUE NOT NULL,
    full_name VARCHAR(100),
    hashed_password VARCHAR(255),
    role VARCHAR(20) CHECK (role IN ('admin', 'manager', 'analyst', 'viewer')),
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE audit_logs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id),
    action VARCHAR(50),
    resource_type VARCHAR(50),
    resource_id VARCHAR(100),
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    ip_address INET,
    details JSONB
);
```

#### Migrated Data Schema (SQLite)

```sql
-- Assets database (assets.db)
CREATE TABLE assets (
    id INTEGER PRIMARY KEY,
    borrower VARCHAR(100),
    industry VARCHAR(50),
    par_amount DECIMAL(15,2),
    current_balance DECIMAL(15,2),
    rating VARCHAR(10),
    maturity_date DATE,
    spread DECIMAL(8,4),
    -- ... additional 64 columns
);

-- Correlations database (correlations.db)
CREATE TABLE asset_correlations (
    id INTEGER PRIMARY KEY,
    borrower_1 VARCHAR(100),
    borrower_2 VARCHAR(100),
    correlation_coefficient DECIMAL(10,8),
    INDEX idx_borrower1 (borrower_1),
    INDEX idx_borrower2 (borrower_2)
);
```

### Data Flow Patterns

#### Read Operations
```
API Request → Service Layer → Data Integration → SQLite/PostgreSQL → Cache → Response
```

#### Write Operations  
```
API Request → Validation → Service Layer → PostgreSQL → Audit Log → Response
```

#### Batch Operations
```
Scheduled Job → Data Integration → Bulk Processing → Cache Invalidation → Monitoring
```

---

## Application Architecture

### Layered Architecture Pattern

```
┌─────────────────────────────────────────────────────────┐
│                 Presentation Layer                      │
│  - FastAPI routes                                       │
│  - Request/Response handling                            │
│  - Authentication middleware                            │
└─────────────────────────────────────────────────────────┘
                              │
┌─────────────────────────────────────────────────────────┐
│                 Business Logic Layer                    │
│  - Portfolio management                                 │
│  - Risk calculations                                    │
│  - Waterfall processing                                 │
│  - Scenario analysis                                    │
└─────────────────────────────────────────────────────────┘
                              │
┌─────────────────────────────────────────────────────────┐
│                 Data Access Layer                       │
│  - Data integration service                             │
│  - Database connections                                 │
│  - Cache management                                     │
│  - Query optimization                                   │
└─────────────────────────────────────────────────────────┘
                              │
┌─────────────────────────────────────────────────────────┐
│                 Infrastructure Layer                    │
│  - Database servers                                     │
│  - Cache servers                                        │
│  - File systems                                         │
│  - External APIs                                        │
└─────────────────────────────────────────────────────────┘
```

### Design Patterns

#### Dependency Injection
```python
# FastAPI dependency system
from fastapi import Depends

def get_auth_service() -> AuthService:
    return AuthService()

def get_portfolio_service() -> PortfolioService:
    return PortfolioService()

@app.get("/api/v1/portfolios")
async def list_portfolios(
    current_user: User = Depends(get_current_user),
    portfolio_service: PortfolioService = Depends(get_portfolio_service)
):
    return await portfolio_service.list_portfolios(current_user)
```

#### Repository Pattern
```python
# Abstract repository interface
from abc import ABC, abstractmethod

class AssetRepository(ABC):
    @abstractmethod
    def get_by_id(self, asset_id: str) -> Optional[Asset]:
        pass
    
    @abstractmethod
    def list_by_portfolio(self, portfolio_id: str) -> List[Asset]:
        pass

# Concrete implementation
class SQLiteAssetRepository(AssetRepository):
    def get_by_id(self, asset_id: str) -> Optional[Asset]:
        # Implementation using SQLite connection
        pass
```

#### Service Layer Pattern
```python
# Business logic encapsulation
class PortfolioService:
    def __init__(
        self,
        asset_repo: AssetRepository,
        portfolio_repo: PortfolioRepository,
        risk_service: RiskService
    ):
        self.asset_repo = asset_repo
        self.portfolio_repo = portfolio_repo
        self.risk_service = risk_service
    
    def create_portfolio(self, portfolio_data: PortfolioCreate) -> Portfolio:
        # Business logic for portfolio creation
        portfolio = self.portfolio_repo.create(portfolio_data)
        self.risk_service.calculate_initial_metrics(portfolio)
        return portfolio
```

---

## Integration Patterns

### API Integration

#### RESTful API Design
```python
# Standard REST endpoints
@app.get("/api/v1/portfolios")           # List portfolios
@app.post("/api/v1/portfolios")          # Create portfolio
@app.get("/api/v1/portfolios/{id}")      # Get portfolio
@app.put("/api/v1/portfolios/{id}")      # Update portfolio
@app.delete("/api/v1/portfolios/{id}")   # Delete portfolio

# Nested resources
@app.get("/api/v1/portfolios/{id}/assets")        # Portfolio assets
@app.post("/api/v1/portfolios/{id}/waterfall")    # Calculate waterfall
```

#### Response Standardization
```python
# Consistent response format
from pydantic import BaseModel

class APIResponse[T](BaseModel):
    success: bool
    data: Optional[T] = None
    message: str = ""
    errors: List[str] = []
    metadata: Dict[str, Any] = {}

# Usage
@app.get("/api/v1/portfolios", response_model=APIResponse[List[Portfolio]])
async def list_portfolios():
    portfolios = await portfolio_service.list_all()
    return APIResponse(
        success=True,
        data=portfolios,
        metadata={"total": len(portfolios)}
    )
```

### Database Integration

#### Connection Management
```python
# Database configuration
class DatabaseConfig:
    def __init__(self):
        self.postgresql_url = settings.database_url
        self.sqlite_paths = self._get_sqlite_paths()
        self.redis_url = settings.redis_url
    
    @contextmanager
    def get_db_session(self):
        session = SessionLocal()
        try:
            yield session
        finally:
            session.close()
    
    def get_sqlite_connection(self, db_name: str):
        return sqlite3.connect(self.sqlite_paths[db_name])
```

#### Data Synchronization
```python
# Periodic data synchronization
@scheduler.scheduled_job('interval', minutes=30)
def sync_portfolio_data():
    integration_service = DataIntegrationService()
    
    # Sync asset data
    integration_service.sync_assets_to_operational()
    
    # Update correlation cache
    integration_service.refresh_correlation_cache()
    
    # Validate data integrity
    integration_service.validate_data_consistency()
```

### External System Integration

#### Third-Party Data Sources
```python
# Market data integration
class MarketDataAdapter:
    def __init__(self, provider: str):
        self.provider = provider
        self.client = self._init_client()
    
    async def fetch_asset_prices(self, identifiers: List[str]):
        # Fetch from external API
        pass
    
    async def fetch_rating_changes(self, date_from: datetime):
        # Monitor rating agency updates
        pass
```

---

## Security Architecture

### Authentication & Authorization

#### JWT-Based Authentication
```python
# Token-based authentication flow
class AuthService:
    def create_access_token(self, user_data: dict) -> str:
        expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
        to_encode = {"sub": user_data["email"], "exp": expire}
        return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    
    def verify_token(self, token: str) -> Optional[User]:
        try:
            payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
            return self.get_user_by_email(payload.get("sub"))
        except JWTError:
            return None
```

#### Role-Based Access Control
```python
# Permission system
class UserRole(Enum):
    ADMIN = "admin"
    MANAGER = "manager"
    ANALYST = "analyst"
    VIEWER = "viewer"

class Permission(Enum):
    READ_PORTFOLIOS = "read:portfolios"
    WRITE_PORTFOLIOS = "write:portfolios"
    CALCULATE_WATERFALL = "execute:waterfall"
    MANAGE_USERS = "manage:users"

# Permission checks
def require_permission(permission: Permission):
    def decorator(func):
        def wrapper(*args, **kwargs):
            current_user = get_current_user()
            if not has_permission(current_user, permission):
                raise HTTPException(status_code=403, detail="Insufficient permissions")
            return func(*args, **kwargs)
        return wrapper
    return decorator
```

### Data Security

#### Encryption at Rest
```python
# Sensitive data encryption
from cryptography.fernet import Fernet

class DataEncryption:
    def __init__(self):
        self.fernet = Fernet(settings.encryption_key)
    
    def encrypt_sensitive_data(self, data: str) -> str:
        return self.fernet.encrypt(data.encode()).decode()
    
    def decrypt_sensitive_data(self, encrypted_data: str) -> str:
        return self.fernet.decrypt(encrypted_data.encode()).decode()
```

#### Input Validation
```python
# Comprehensive input validation
from pydantic import BaseModel, validator

class PortfolioCreate(BaseModel):
    name: str
    total_commitments: Decimal
    
    @validator('name')
    def validate_name(cls, v):
        if not v or len(v.strip()) == 0:
            raise ValueError('Portfolio name cannot be empty')
        return v.strip()
    
    @validator('total_commitments')
    def validate_commitments(cls, v):
        if v <= 0:
            raise ValueError('Total commitments must be positive')
        return v
```

### Audit & Monitoring

#### Comprehensive Audit Trail
```python
# Audit logging system
class AuditLogger:
    def log_action(
        self,
        user_id: str,
        action: str,
        resource_type: str,
        resource_id: str,
        details: Dict[str, Any] = None
    ):
        audit_entry = AuditLog(
            user_id=user_id,
            action=action,
            resource_type=resource_type,
            resource_id=resource_id,
            timestamp=datetime.utcnow(),
            details=details or {}
        )
        
        db.session.add(audit_entry)
        db.session.commit()
```

---

## Deployment Architecture

### Containerized Deployment

#### Docker Configuration
```dockerfile
# Multi-stage production build
FROM python:3.9-slim as builder
WORKDIR /app
COPY requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

FROM python:3.9-slim as production
WORKDIR /app
COPY --from=builder /root/.local /root/.local
COPY . .
ENV PATH=/root/.local/bin:$PATH
EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### Docker Compose Stack
```yaml
version: '3.8'
services:
  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:pass@db:5432/clo_management
      - REDIS_URL=redis://cache:6379
    depends_on:
      - db
      - cache
  
  db:
    image: postgres:13
    environment:
      - POSTGRES_DB=clo_management
      - POSTGRES_USER=clo_user
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
  
  cache:
    image: redis:6-alpine
    volumes:
      - redis_data:/data
  
  monitoring:
    image: grafana/grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
```

### Infrastructure as Code

#### Environment Configuration
```python
# Environment-specific settings
class Settings:
    def __init__(self):
        self.app_name = os.getenv("APP_NAME", "CLO Management System")
        self.debug = os.getenv("DEBUG", "False").lower() == "true"
        self.database_url = os.getenv("DATABASE_URL")
        self.redis_url = os.getenv("REDIS_URL")
        self.secret_key = os.getenv("SECRET_KEY")
        self.log_level = os.getenv("LOG_LEVEL", "INFO")
```

### High Availability Design

#### Load Balancing
```nginx
# nginx configuration
upstream app_servers {
    server app1:8000 weight=3;
    server app2:8000 weight=3;
    server app3:8000 weight=2;
}

server {
    listen 80;
    server_name clo-system.company.com;
    
    location /api/ {
        proxy_pass http://app_servers;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

#### Database Clustering
```yaml
# PostgreSQL high availability
services:
  db-primary:
    image: postgres:13
    environment:
      - POSTGRES_REPLICATION_MODE=master
      - POSTGRES_REPLICATION_USER=replicator
  
  db-replica:
    image: postgres:13
    environment:
      - POSTGRES_REPLICATION_MODE=slave
      - POSTGRES_MASTER_SERVICE=db-primary
    depends_on:
      - db-primary
```

---

## Performance Considerations

### Caching Strategy

#### Multi-Level Caching
```python
# Comprehensive caching implementation
class CacheManager:
    def __init__(self):
        self.redis_client = redis.Redis.from_url(settings.redis_url)
        self.local_cache = {}  # In-memory cache
    
    async def get_or_set(self, key: str, factory_func, ttl: int = 3600):
        # Level 1: In-memory cache
        if key in self.local_cache:
            return self.local_cache[key]
        
        # Level 2: Redis cache
        cached_value = self.redis_client.get(key)
        if cached_value:
            value = json.loads(cached_value)
            self.local_cache[key] = value
            return value
        
        # Level 3: Generate and cache
        value = await factory_func()
        self.redis_client.setex(key, ttl, json.dumps(value))
        self.local_cache[key] = value
        return value
```

### Database Optimization

#### Query Optimization
```sql
-- Strategic index creation
CREATE INDEX CONCURRENTLY idx_assets_rating_industry ON assets(rating, industry);
CREATE INDEX CONCURRENTLY idx_correlations_composite ON asset_correlations(borrower_1, borrower_2);
CREATE INDEX CONCURRENTLY idx_audit_logs_user_timestamp ON audit_logs(user_id, timestamp DESC);

-- Materialized views for complex calculations
CREATE MATERIALIZED VIEW portfolio_summary AS
SELECT 
    p.id,
    p.name,
    COUNT(pa.asset_id) as asset_count,
    SUM(a.par_amount) as total_par,
    AVG(CASE WHEN a.rating ~ '^[A-C][a-c]*[1-3]?$' THEN 
        CASE a.rating 
            WHEN 'Aaa' THEN 1
            WHEN 'Aa1' THEN 2
            -- ... rating mappings
        END 
    END) as weighted_avg_rating
FROM portfolios p
LEFT JOIN portfolio_assets pa ON p.id = pa.portfolio_id
LEFT JOIN assets a ON pa.asset_id = a.id
GROUP BY p.id, p.name;

-- Refresh materialized views periodically
REFRESH MATERIALIZED VIEW CONCURRENTLY portfolio_summary;
```

#### Connection Pooling
```python
# Database connection pooling
from sqlalchemy.pool import QueuePool

engine = create_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=20,
    max_overflow=30,
    pool_pre_ping=True,
    pool_recycle=3600
)
```

### Asynchronous Processing

#### Background Task Processing
```python
# Celery task queue for heavy operations
from celery import Celery

celery_app = Celery('clo_system')

@celery_app.task
def calculate_portfolio_waterfall(portfolio_id: str):
    """Background waterfall calculation"""
    service = WaterfallService()
    result = service.calculate_waterfall(portfolio_id)
    
    # Cache results
    cache_key = f"waterfall_result:{portfolio_id}"
    cache_manager.set(cache_key, result, ttl=3600)
    
    return result

# Async API endpoint
@app.post("/api/v1/waterfall/calculate/{portfolio_id}")
async def calculate_waterfall_async(portfolio_id: str):
    task = calculate_portfolio_waterfall.delay(portfolio_id)
    return {"task_id": task.id, "status": "processing"}
```

### Monitoring & Observability

#### Application Performance Monitoring
```python
# Prometheus metrics integration
from prometheus_client import Counter, Histogram, Gauge

# Define metrics
REQUEST_COUNT = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint'])
REQUEST_DURATION = Histogram('http_request_duration_seconds', 'HTTP request duration')
ACTIVE_CONNECTIONS = Gauge('database_connections_active', 'Active database connections')

# Middleware for metrics collection
@app.middleware("http")
async def add_process_time_header(request: Request, call_next):
    start_time = time.time()
    
    REQUEST_COUNT.labels(method=request.method, endpoint=request.url.path).inc()
    
    with REQUEST_DURATION.time():
        response = await call_next(request)
    
    process_time = time.time() - start_time
    response.headers["X-Process-Time"] = str(process_time)
    
    return response
```

This technical architecture document provides a comprehensive overview of the CLO Management System's design, enabling developers and architects to understand the system structure and make informed decisions about future enhancements.